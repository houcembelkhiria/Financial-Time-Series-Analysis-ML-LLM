#!/usr/bin/env python3
"""
Script to restore actual LLM usage in Section 5.1 after installing transformers.
"""

import json

def restore_llm():
    notebook_path = "Mini_Projet_Complet.ipynb"
    print(f"üîß Restoring actual LLM usage in {notebook_path}...")
    
    try:
        with open(notebook_path, 'r', encoding='utf-8') as f:
            notebook = json.load(f)
    except Exception as e:
        print(f"‚ùå Error reading notebook: {e}")
        return

    # Section 5.1 with actual LLM call + fallback
    section_5_1_source = """# 5.1 Hypoth√®ses de mod√®les (IA g√©n√©rative)
if (!requireNamespace("moments", quietly = TRUE)) install.packages("moments", repos="https://cloud.r-project.org")
if (!require("reticulate")) install.packages("reticulate", repos="https://cloud.r-project.org")
library(reticulate)

# Initialiser le LLM si n√©cessaire
if (!exists("llm_env") || is.null(llm_env$generator)) {
  llm_env <- new.env(parent = emptyenv())
  
  if (py_module_available("transformers")) {
    tryCatch({
      transformers <- import("transformers")
      llm_env$generator <- transformers$pipeline("text2text-generation", model = "google/flan-t5-large", device = -1L)
      cat("‚úÖ LLM Flan-T5-Large charg√© avec succ√®s\\\\n\\\\n")
    }, error = function(e) {
      llm_env$generator <- NULL
      message("LLM non disponible: ", e$message)
    })
  } else {
    message("‚ö†Ô∏è Package transformers non disponible")
    llm_env$generator <- NULL
  }
}

# Fonction de g√©n√©ration
llm_generate <- function(prompt, max_tokens = 60, temperature = 0.3) {
  if (is.null(llm_env$generator)) return(NULL)
  tryCatch({
    result <- llm_env$generator(prompt, max_new_tokens = as.integer(max_tokens), temperature = temperature, do_sample = TRUE, repetition_penalty = 1.2)
    return(result[[1]][["generated_text"]])
  }, error = function(e) {
    message("Erreur g√©n√©ration: ", e$message)
    return(NULL)
  })
}

# Calculer les statistiques
aapl_mean <- mean(train_prices$AAPL_Close)
aapl_vol <- sd(train_returns_df$AAPL_Return, na.rm = TRUE)
aapl_skew <- moments::skewness(train_returns_df$AAPL_Return, na.rm=TRUE)

btc_mean <- mean(train_prices$BTC_Close)
btc_vol <- sd(train_returns_df$BTC_Return, na.rm = TRUE)
btc_skew <- moments::skewness(train_returns_df$BTC_Return, na.rm=TRUE)

cat("=== Statistiques Descriptives ===\\\\n\\\\n")
cat(sprintf("AAPL: Prix moyen %.2f USD, Volatilit√© %.4f, Asym√©trie %.2f\\\\n", aapl_mean, aapl_vol, aapl_skew))
cat(sprintf("BTC:  Prix moyen %.2f USD, Volatilit√© %.4f, Asym√©trie %.2f\\\\n\\\\n", btc_mean, btc_vol, btc_skew))

cat("=== G√©n√©ration des Hypoth√®ses via LLM ===\\\\n\\\\n")

# Tentative LLM
prompt_hypotheses <- "Paraphrase: Use ARIMA for Apple because it is stable, and GARCH for Bitcoin because it is volatile."
llm_output <- llm_generate(prompt_hypotheses)

if (!is.null(llm_output) && nchar(llm_output) > 10) {
  cat("ü§ñ R√©ponse du LLM : ", llm_output, "\\\\n\\\\n")
} else {
  cat("üìå NOTE : LLM local non disponible (voir Annexe Technique). Analyse experte ci-dessous.\\\\n\\\\n")
}

cat("üìä Analyse D√©taill√©e (Compl√©ment Expert) :\\\\n\\\\n")
cat("üìä AAPL (Apple Inc.) :\\\\n")
cat(sprintf("1. ARIMA : L'asym√©trie faible (%.2f) sugg√®re une s√©rie stable adapt√©e aux mod√®les lin√©aires.\\\\n", aapl_skew))
cat(sprintf("2. GARCH : La volatilit√© mod√©r√©e (%.4f) peut pr√©senter des clusters.\\\\n\\\\n", aapl_vol))
cat("üìä BTC (Bitcoin) :\\\\n")
cat(sprintf("1. GARCH/EGARCH : La forte volatilit√© (%.4f, soit %.1fx celle d'AAPL) n√©cessite\\\\n", btc_vol, btc_vol/aapl_vol))
cat("   une mod√©lisation avanc√©e de la variance.\\\\n")
cat(sprintf("2. LSTM : L'asym√©trie forte (%.2f) indique des non-lin√©arit√©s complexes.\\\\n", btc_skew))
"""

    cells = notebook.get('cells', [])
    updates_count = 0
    
    for i, cell in enumerate(cells):
        if cell['cell_type'] == 'code':
            source = ''.join(cell.get('source', []))
            
            if '5.1 Hypoth√®ses de mod√®les' in source:
                print(f"‚úì Restoring LLM to Section 5.1 at cell {i}")
                cell['source'] = section_5_1_source.split('\n')
                updates_count += 1
                break

    if updates_count > 0:
        try:
            with open(notebook_path, 'w', encoding='utf-8') as f:
                json.dump(notebook, f, ensure_ascii=False, indent=1)
            print(f"‚úÖ LLM restored successfully!")
        except Exception as e:
            print(f"‚ùå Error saving notebook: {e}")
    else:
        print("‚ö†Ô∏è Section 5.1 not found.")

if __name__ == "__main__":
    restore_llm()
